{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers å¾®è°ƒè¯­è¨€æ¨¡å‹-é—®ç­”ä»»åŠ¡\n",
    "\n",
    "æˆ‘ä»¬å·²ç»å­¦ä¼šä½¿ç”¨ Pipeline åŠ è½½æ”¯æŒé—®ç­”ä»»åŠ¡çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œæœ¬æ•™ç¨‹ä»£ç å°†å±•ç¤ºå¦‚ä½•å¾®è°ƒè®­ç»ƒä¸€ä¸ªæ”¯æŒé—®ç­”ä»»åŠ¡çš„æ¨¡å‹ã€‚\n",
    "\n",
    "**æ³¨æ„ï¼šå¾®è°ƒåçš„æ¨¡å‹ä»ç„¶æ˜¯é€šè¿‡æå–ä¸Šä¸‹æ–‡çš„å­ä¸²æ¥å›ç­”é—®é¢˜çš„ï¼Œè€Œä¸æ˜¯ç”Ÿæˆæ–°çš„æ–‡æœ¬ã€‚**\n",
    "\n",
    "### æ¨¡å‹æ‰§è¡Œé—®ç­”æ•ˆæœç¤ºä¾‹\n",
    "\n",
    "![Widget inference representing the QA task](docs/images/question_answering.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zVvslsfMIrIh"
   },
   "outputs": [],
   "source": [
    "# æ ¹æ®ä½ ä½¿ç”¨çš„æ¨¡å‹å’ŒGPUèµ„æºæƒ…å†µï¼Œè°ƒæ•´ä»¥ä¸‹å…³é”®å‚æ•°\n",
    "squad_v2 = False\n",
    "model_checkpoint = \"distilbert/distilbert-base-uncased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## ä¸‹è½½æ•°æ®é›†\n",
    "\n",
    "åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[æ–¯å¦ç¦é—®ç­”æ•°æ®é›†(SQuADï¼‰](https://rajpurkar.github.io/SQuAD-explorer/)ã€‚\n",
    "\n",
    "### SQuAD æ•°æ®é›†\n",
    "\n",
    "**æ–¯å¦ç¦é—®ç­”æ•°æ®é›†(SQuAD)** æ˜¯ä¸€ä¸ªé˜…è¯»ç†è§£æ•°æ®é›†ï¼Œç”±ä¼—åŒ…å·¥ä½œè€…åœ¨ä¸€ç³»åˆ—ç»´åŸºç™¾ç§‘æ–‡ç« ä¸Šæå‡ºé—®é¢˜ç»„æˆã€‚æ¯ä¸ªé—®é¢˜çš„ç­”æ¡ˆéƒ½æ˜¯ç›¸åº”é˜…è¯»æ®µè½ä¸­çš„æ–‡æœ¬ç‰‡æ®µæˆ–èŒƒå›´ï¼Œæˆ–è€…è¯¥é—®é¢˜å¯èƒ½æ— æ³•å›ç­”ã€‚\n",
    "\n",
    "SQuAD2.0å°†SQuAD1.1ä¸­çš„10ä¸‡ä¸ªé—®é¢˜ä¸ç”±ä¼—åŒ…å·¥ä½œè€…å¯¹æŠ—æ€§åœ°æ’°å†™çš„5ä¸‡å¤šä¸ªæ— æ³•å›ç­”çš„é—®é¢˜ç›¸ç»“åˆï¼Œä½¿å…¶çœ‹èµ·æ¥ä¸å¯å›ç­”çš„é—®é¢˜ç±»ä¼¼ã€‚è¦åœ¨SQuAD2.0ä¸Šè¡¨ç°è‰¯å¥½ï¼Œç³»ç»Ÿä¸ä»…å¿…é¡»åœ¨å¯èƒ½æ—¶å›ç­”é—®é¢˜ï¼Œè¿˜å¿…é¡»ç¡®å®šæ®µè½ä¸­æ²¡æœ‰æ”¯æŒä»»ä½•ç­”æ¡ˆï¼Œå¹¶æ”¾å¼ƒå›ç­”ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IreSlFmlIrIm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 7.62kB [00:00, 3.70MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14.5M/14.5M [00:01<00:00, 13.5MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.82M/1.82M [00:00<00:00, 2.70MB/s]\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87599/87599 [00:00<00:00, 374979.42 examples/s]\n",
      "Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10570/10570 [00:00<00:00, 357409.53 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "datasets = load_dataset(\"squad_v2\" if squad_v2 else \"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzfPtOMoIrIu"
   },
   "source": [
    "The `datasets` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GWiVUF0jIrIv",
    "outputId": "35e3ea43-f397-4a54-c90c-f2cf8d36873e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å¯¹æ¯”æ•°æ®é›†\n",
    "\n",
    "ç›¸æ¯”å¿«é€Ÿå…¥é—¨ä½¿ç”¨çš„ Yelp è¯„è®ºæ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° SQuAD è®­ç»ƒå’Œæµ‹è¯•é›†éƒ½æ–°å¢äº†ç”¨äºä¸Šä¸‹æ–‡ã€é—®é¢˜ä»¥åŠé—®é¢˜ç­”æ¡ˆçš„åˆ—ï¼š\n",
    "\n",
    "**YelpReviewFull Datasetï¼š**\n",
    "\n",
    "```json\n",
    "\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['label', 'text'],\n",
    "        num_rows: 650000\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['label', 'text'],\n",
    "        num_rows: 50000\n",
    "    })\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "X6HrpprwIrIz",
    "outputId": "d7670bc0-42e4-4c09-8a6a-5c018ded7d95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ä»ä¸Šä¸‹æ–‡ä¸­ç»„ç»‡å›å¤å†…å®¹\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç­”æ¡ˆæ˜¯é€šè¿‡å®ƒä»¬åœ¨æ–‡æœ¬ä¸­çš„èµ·å§‹ä½ç½®ï¼ˆè¿™é‡Œæ˜¯ç¬¬515ä¸ªå­—ç¬¦ï¼‰ä»¥åŠå®ƒä»¬çš„å®Œæ•´æ–‡æœ¬è¡¨ç¤ºçš„ï¼Œè¿™æ˜¯ä¸Šé¢æåˆ°çš„ä¸Šä¸‹æ–‡çš„å­å­—ç¬¦ä¸²ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "i3j8APAoIrI3"
   },
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "SZy5tRB_IrI7",
    "outputId": "ba8f2124-e485-488f-8c0c-254f34f24f13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57314067a5e9cc1400cdbdc2</td>\n",
       "      <td>Red</td>\n",
       "      <td>As a ray of white sunlight travels through the atmosphere to the eye, some of the colors are scattered out of the beam by air molecules and airborne particles due to Rayleigh scattering, changing the final color of the beam that is seen. Colors with a shorter wavelength, such as blue and green, scatter more strongly, and are removed from the light that finally reaches the eye. At sunrise and sunset, when the path of the sunlight through the atmosphere to the eye is longest, the blue and green components are removed almost completely, leaving the longer wavelength orange and red light. The remaining reddened sunlight can also be scattered by cloud droplets and other relatively large particles, which give the sky above the horizon its red glow.</td>\n",
       "      <td>In comparison to blue and green, what would describe the wavelength of orange or red light?</td>\n",
       "      <td>{'text': ['longer'], 'answer_start': [552]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>572944df6aef051400154c24</td>\n",
       "      <td>Race_(human_categorization)</td>\n",
       "      <td>Mass incarceration in the United States disproportionately impacts African American and Latino communities. Michelle Alexander, author of The New Jim Crow: Mass Incarceration in the Age of Colorblindness (2010), argues that mass incarceration is best understood as not only a system of overcrowded prisons. Mass incarceration is also, \"the larger web of laws, rules, policies, and customs that control those labeled criminals both in and out of prison.\" She defines it further as \"a system that locks people not only behind actual bars in actual prisons, but also behind virtual bars and virtual walls\", illustrating the second-class citizenship that is imposed on a disproportionate number of people of color, specifically African-Americans. She compares mass incarceration to Jim Crow laws, stating that both work as racial caste systems.</td>\n",
       "      <td>What impacts African American and Latino communities in disproportionate numbers?</td>\n",
       "      <td>{'text': ['Mass incarceration'], 'answer_start': [0]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57341cfc4776f419006618a3</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Many Portuguese holidays, festivals and traditions have a Christian origin or connotation. Although relations between the Portuguese state and the Roman Catholic Church were generally amiable and stable since the earliest years of the Portuguese nation, their relative power fluctuated. In the 13th and 14th centuries, the church enjoyed both riches and power stemming from its role in the reconquest, its close identification with early Portuguese nationalism and the foundation of the Portuguese educational system, including the first university. The growth of the Portuguese overseas empire made its missionaries important agents of colonization, with important roles in the education and evangelization of people from all the inhabited continents. The growth of liberal and nascent republican movements during the eras leading to the formation of the First Portuguese Republic (1910â€“26) changed the role and importance of organized religion.</td>\n",
       "      <td>What changed the role and importance of organized religion in Portugal?</td>\n",
       "      <td>{'text': ['formation of the First Portuguese Republic'], 'answer_start': [839]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57282b433acd2414000df609</td>\n",
       "      <td>Gamal_Abdel_Nasser</td>\n",
       "      <td>Nasser also attempted to maintain oversight of the country's civil service to prevent it from inflating and consequently becoming a burden to the state. New laws provided workers with a minimum wage, profit shares, free education, free health care, reduced working hours, and encouragement to participate in management. Land reforms guaranteed the security of tenant farmers, promoted agricultural growth, and reduced rural poverty. As a result of the 1962 measures, government ownership of Egyptian business reached 51 percent, and the National Union was renamed the Arab Socialist Union (ASU). With these measures came more domestic repression, as thousands of Islamists were imprisoned, including dozens of military officers. Nasser's tilt toward a Soviet-style system led his aides Boghdadi and Hussein el-Shafei to submit their resignations in protest.</td>\n",
       "      <td>How did two of Nasser's top aids respond to the new measures?</td>\n",
       "      <td>{'text': ['submit their resignations in protest'], 'answer_start': [820]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>573112da05b4da19006bcd48</td>\n",
       "      <td>United_States_Air_Force</td>\n",
       "      <td>Officers may be commissioned upon graduation from the United States Air Force Academy, upon graduation from another college or university through the Air Force Reserve Officer Training Corps (AFROTC) program, or through the Air Force Officer Training School (OTS). OTS, previously located at Lackland AFB, Texas until 1993 and located at Maxwell Air Force Base in Montgomery, Alabama since 1993, in turn encompasses two separate commissioning programs: Basic Officer Training (BOT), which is for line-officer candidates of the active-duty Air Force and the U.S. Air Force Reserve; and the Academy of Military Science (AMS), which is for line-officer candidates of the Air National Guard. (The term \"line officer\" derives from the concept of the line of battle and refers to an officer whose role falls somewhere within the \"Line of the Air\", meaning combat or combat-support operations within the scope of legitimate combatants as defined by the Geneva Conventions.)</td>\n",
       "      <td>What is a line officer in the USAF?</td>\n",
       "      <td>{'text': ['concept of the line of battle and refers to an officer whose role falls somewhere within the \"Line of the Air\"'], 'answer_start': [730]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5728c3643acd2414000dfdd0</td>\n",
       "      <td>Apollo</td>\n",
       "      <td>The function of Apollo as a \"healer\" is connected with Paean (Î Î±Î¹ÏÎ½-Î Î±Î¹Î®Ï‰Î½), the physician of the Gods in the Iliad, who seems to come from a more primitive religion. PaeÎ¿n is probably connected with the Mycenean pa-ja-wo-ne (Linear B: ğ€ğ€Šğ€ğ€š), but this is not certain. He did not have a separate cult, but he was the personification of the holy magic-song sung by the magicians that was supposed to cure disease. Later the Greeks knew the original meaning of the relevant song \"paean\" (Ï€Î±Î¹Î¬Î½). The magicians were also called \"seer-doctors\" (á¼°Î±Ï„ÏÎ¿Î¼Î¬Î½Ï„ÎµÎ¹Ï‚), and they used an ecstatic prophetic art which was used exactly by the god Apollo at the oracles.</td>\n",
       "      <td>Who is the physician of the Gods in the Iliad?</td>\n",
       "      <td>{'text': ['Paean'], 'answer_start': [55]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>570d257ab3d812140066d485</td>\n",
       "      <td>Valencia</td>\n",
       "      <td>In the early 20th century Valencia was an industrialised city. The silk industry had disappeared, but there was a large production of hides and skins, wood, metals and foodstuffs, this last with substantial exports, particularly of wine and citrus. Small businesses predominated, but with the rapid mechanisation of industry larger companies were being formed. The best expression of this dynamic was in the regional exhibitions, including that of 1909 held next to the pedestrian avenue L'Albereda (Paseo de la Alameda), which depicted the progress of agriculture and industry. Among the most architecturally successful buildings of the era were those designed in the Art Nouveau style, such as the North Station (Gare du Nord) and the Central and Columbus markets.</td>\n",
       "      <td>What were Valencia's main food exports in the early 20th century?</td>\n",
       "      <td>{'text': ['wine and citrus'], 'answer_start': [232]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5726aabcf1498d1400e8e688</td>\n",
       "      <td>Somerset</td>\n",
       "      <td>Population growth is higher than the national average, with a 6.4% increase, in the Somerset County Council area, since 1991, and a 17% increase since 1981. The population density is 1.4 persons per hectare, which can be compared to 2.07 persons per hectare for the South West region. Within the county, population density ranges 0.5 in West Somerset to 2.2 persons per hectare in Taunton Deane. The percentage of the population who are economically active is higher than the regional and national average, and the unemployment rate is lower than the regional and national average.</td>\n",
       "      <td>What percentage of the population is economically active</td>\n",
       "      <td>{'text': ['is higher than the regional and national average'], 'answer_start': [457]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>56e02a437aa994140058e2de</td>\n",
       "      <td>Comics</td>\n",
       "      <td>From the 1980s, mainstream sensibilities were reasserted and serialization became less common as the number of comics magazines decreased and many comics began to be published directly as albums. Smaller publishers such as L'Association that published longer works in non-traditional formats by auteur-istic creators also became common. Since the 1990s, mergers resulted in fewer large publishers, while smaller publishers proliferated. Sales overall continued to grow despite the trend towards a shrinking print market.</td>\n",
       "      <td>How were comics published when serialization became less common?</td>\n",
       "      <td>{'text': ['as albums'], 'answer_start': [185]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>56e7299137bdd419002c3ddd</td>\n",
       "      <td>Daylight_saving_time</td>\n",
       "      <td>Starting on 30 April 1916, Germany and its World War I ally Austria-Hungary were the first to use DST (German: Sommerzeit) as a way to conserve coal during wartime. Britain, most of its allies, and many European neutrals soon followed suit. Russia and a few other countries waited until the next year and the United States adopted it in 1918.</td>\n",
       "      <td>What natural resource were Germany and Austria-Hungary trying to conserve by using DST?</td>\n",
       "      <td>{'text': ['coal'], 'answer_start': [144]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## é¢„å¤„ç†æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "eXNLu_-nIrJI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vl6IidfdIrJK"
   },
   "source": [
    "ä»¥ä¸‹æ–­è¨€ç¡®ä¿æˆ‘ä»¬çš„ Tokenizers ä½¿ç”¨çš„æ˜¯ FastTokenizerï¼ˆRust å®ç°ï¼Œé€Ÿåº¦å’ŒåŠŸèƒ½æ€§ä¸Šæœ‰ä¸€å®šä¼˜åŠ¿ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‚¨å¯ä»¥åœ¨å¤§æ¨¡å‹è¡¨ä¸ŠæŸ¥çœ‹å“ªç§ç±»å‹çš„æ¨¡å‹å…·æœ‰å¯ç”¨çš„å¿«é€Ÿæ ‡è®°å™¨ï¼Œå“ªç§ç±»å‹æ²¡æœ‰ã€‚\n",
    "\n",
    "æ‚¨å¯ä»¥ç›´æ¥åœ¨ä¸¤ä¸ªå¥å­ä¸Šè°ƒç”¨æ­¤æ ‡è®°å™¨ï¼ˆä¸€ä¸ªç”¨äºç­”æ¡ˆï¼Œä¸€ä¸ªç”¨äºä¸Šä¸‹æ–‡ï¼‰ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "a5hBlsrHIrJL",
    "outputId": "acdaa98a-a8cd-4a20-89b8-cc26437bbe90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2054, 2003, 2115, 2171, 1029, 102, 2026, 2171, 2003, 25353, 22144, 2378, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"What is your name?\", \"My name is Sylvain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer è¿›é˜¶æ“ä½œ\n",
    "\n",
    "åœ¨é—®ç­”é¢„å¤„ç†ä¸­çš„ä¸€ä¸ªç‰¹å®šé—®é¢˜æ˜¯å¦‚ä½•å¤„ç†éå¸¸é•¿çš„æ–‡æ¡£ã€‚\n",
    "\n",
    "åœ¨å…¶ä»–ä»»åŠ¡ä¸­ï¼Œå½“æ–‡æ¡£çš„é•¿åº¦è¶…è¿‡æ¨¡å‹æœ€å¤§å¥å­é•¿åº¦æ—¶ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šæˆªæ–­å®ƒä»¬ï¼Œä½†åœ¨è¿™é‡Œï¼Œåˆ é™¤ä¸Šä¸‹æ–‡çš„ä¸€éƒ¨åˆ†å¯èƒ½ä¼šå¯¼è‡´æˆ‘ä»¬ä¸¢å¤±æ­£åœ¨å¯»æ‰¾çš„ç­”æ¡ˆã€‚\n",
    "\n",
    "ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å…è®¸æ•°æ®é›†ä¸­çš„ä¸€ä¸ªï¼ˆé•¿ï¼‰ç¤ºä¾‹ç”Ÿæˆå¤šä¸ªè¾“å…¥ç‰¹å¾ï¼Œæ¯ä¸ªç‰¹å¾çš„é•¿åº¦éƒ½å°äºæ¨¡å‹çš„æœ€å¤§é•¿åº¦ï¼ˆæˆ–æˆ‘ä»¬è®¾ç½®çš„è¶…å‚æ•°ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length of a feature (question and context)\n",
    "max_length = 384 \n",
    "# The authorized overlap between two part of the context when splitting it is needed.\n",
    "doc_stride = 128 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### è¶…å‡ºæœ€å¤§é•¿åº¦çš„æ–‡æœ¬æ•°æ®å¤„ç†\n",
    "\n",
    "ä¸‹é¢ï¼Œæˆ‘ä»¬ä»è®­ç»ƒé›†ä¸­æ‰¾å‡ºä¸€ä¸ªè¶…è¿‡æœ€å¤§é•¿åº¦ï¼ˆ384ï¼‰çš„æ–‡æœ¬ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, example in enumerate(datasets[\"train\"]):\n",
    "    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > 384:\n",
    "        break\n",
    "# æŒ‘é€‰å‡ºæ¥è¶…è¿‡384ï¼ˆæœ€å¤§é•¿åº¦ï¼‰çš„æ•°æ®æ ·ä¾‹\n",
    "example = datasets[\"train\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "396"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æˆªæ–­ä¸Šä¸‹æ–‡ä¸ä¿ç•™è¶…å‡ºéƒ¨åˆ†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"],\n",
    "              example[\"context\"],\n",
    "              max_length=max_length,\n",
    "              truncation=\"only_second\")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å…³äºæˆªæ–­çš„ç­–ç•¥\n",
    "\n",
    "- ç›´æ¥æˆªæ–­è¶…å‡ºéƒ¨åˆ†: truncation=`only_second`\n",
    "- ä»…æˆªæ–­ä¸Šä¸‹æ–‡ï¼ˆcontextï¼‰ï¼Œä¿ç•™é—®é¢˜ï¼ˆquestionï¼‰ï¼š`return_overflowing_tokens=True` & è®¾ç½®`stride`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=doc_stride\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨æ­¤ç­–ç•¥æˆªæ–­åï¼ŒTokenizer å°†è¿”å›å¤šä¸ª `input_ids` åˆ—è¡¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[384, 157]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in tokenized_example[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è§£ç ä¸¤ä¸ªè¾“å…¥ç‰¹å¾ï¼Œå¯ä»¥çœ‹åˆ°é‡å çš„éƒ¨åˆ†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 21:09:29.091167: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] how many wins does the notre dame men's basketball team have? [SEP] the men's basketball team has over 1, 600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 ncaa tournaments. former player austin carr holds the record for most points scored in a single game of the tournament with 61. although the team has never won the ncaa tournament, they were named by the helms athletic foundation as national champions twice. the team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending ucla's record 88 - game winning streak in 1974. the team has beaten an additional eight number - one teams, and those nine wins rank second, to ucla's 10, all - time in wins against the top team. the team plays in newly renovated purcell pavilion ( within the edmund p. joyce center ), which reopened for the beginning of the 2009 â€“ 2010 season. the team is coached by mike brey, who, as of the 2014 â€“ 15 season, his fifteenth at notre dame, has achieved a 332 - 165 record. in 2009 they were invited to the nit, where they advanced to the semifinals but were beaten by penn state who went on and beat baylor in the championship. the 2010 â€“ 11 team concluded its regular season ranked number seven in the country, with a record of 25 â€“ 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were [SEP]\n",
      "[CLS] how many wins does the notre dame men's basketball team have? [SEP] championship. the 2010 â€“ 11 team concluded its regular season ranked number seven in the country, with a record of 25 â€“ 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were the most by the fighting irish team since 1908 - 09. [SEP]\n"
     ]
    }
   ],
   "source": [
    "for x in tokenized_example[\"input_ids\"][:2]:\n",
    "    print(tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ä½¿ç”¨ offsets_mapping è·å–åŸå§‹çš„ input_ids\n",
    "\n",
    "è®¾ç½® `return_offsets_mapping=True`ï¼Œå°†ä½¿å¾—æˆªæ–­åˆ†å‰²ç”Ÿæˆçš„å¤šä¸ª input_ids åˆ—è¡¨ä¸­çš„ tokenï¼Œé€šè¿‡æ˜ å°„ä¿ç•™åŸå§‹æ–‡æœ¬çš„ input_idsã€‚\n",
    "\n",
    "å¦‚ä¸‹æ‰€ç¤ºï¼šç¬¬ä¸€ä¸ªæ ‡è®°ï¼ˆ[CLS]ï¼‰çš„èµ·å§‹å’Œç»“æŸå­—ç¬¦éƒ½æ˜¯ï¼ˆ0, 0ï¼‰ï¼Œå› ä¸ºå®ƒä¸å¯¹åº”é—®é¢˜/ç­”æ¡ˆçš„ä»»ä½•éƒ¨åˆ†ï¼Œç„¶åç¬¬äºŒä¸ªæ ‡è®°ä¸é—®é¢˜(question)çš„å­—ç¬¦0åˆ°3ç›¸åŒ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 3), (4, 8), (9, 13), (14, 18), (19, 22), (23, 28), (29, 33), (34, 37), (37, 38), (38, 39), (40, 50), (51, 55), (56, 60), (60, 61), (0, 0), (0, 3), (4, 7), (7, 8), (8, 9), (10, 20), (21, 25), (26, 29), (30, 34), (35, 36), (36, 37), (37, 40), (41, 45), (45, 46), (47, 50), (51, 53), (54, 58), (59, 61), (62, 69), (70, 73), (74, 78), (79, 86), (87, 91), (92, 96), (96, 97), (98, 101), (102, 106), (107, 115), (116, 118), (119, 121), (122, 126), (127, 138), (138, 139), (140, 146), (147, 153), (154, 160), (161, 165), (166, 171), (172, 175), (176, 182), (183, 186), (187, 191), (192, 198), (199, 205), (206, 208), (209, 210), (211, 217), (218, 222), (223, 225), (226, 229), (230, 240), (241, 245), (246, 248), (248, 249), (250, 258), (259, 262), (263, 267), (268, 271), (272, 277), (278, 281), (282, 285), (286, 290), (291, 301), (301, 302), (303, 307), (308, 312), (313, 318), (319, 321), (322, 325), (326, 330), (330, 331), (332, 340), (341, 351), (352, 354), (355, 363), (364, 373), (374, 379), (379, 380), (381, 384), (385, 389), (390, 393), (394, 406), (407, 408), (409, 415), (416, 418)]\n"
     ]
    }
   ],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    stride=doc_stride\n",
    ")\n",
    "print(tokenized_example[\"offset_mapping\"][0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ˜ å°„æ¥æ‰¾åˆ°ç­”æ¡ˆåœ¨ç»™å®šç‰¹å¾ä¸­çš„èµ·å§‹å’Œç»“æŸæ ‡è®°çš„ä½ç½®ã€‚\n",
    "\n",
    "æˆ‘ä»¬åªéœ€åŒºåˆ†åç§»çš„å“ªäº›éƒ¨åˆ†å¯¹åº”äºé—®é¢˜ï¼Œå“ªäº›éƒ¨åˆ†å¯¹åº”äºä¸Šä¸‹æ–‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how How\n"
     ]
    }
   ],
   "source": [
    "first_token_id = tokenized_example[\"input_ids\"][0][1]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][1]\n",
    "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "many many\n"
     ]
    }
   ],
   "source": [
    "second_token_id = tokenized_example[\"input_ids\"][0][2]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][2]\n",
    "print(tokenizer.convert_ids_to_tokens([second_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"How many wins does the Notre Dame men's basketball team have?\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"question\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å€ŸåŠ©`tokenized_example`çš„`sequence_ids`æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥æ–¹ä¾¿çš„åŒºåˆ†tokençš„æ¥æºç¼–å·ï¼š\n",
    "\n",
    "- å¯¹äºç‰¹æ®Šæ ‡è®°ï¼šè¿”å›Noneï¼Œ\n",
    "- å¯¹äºæ­£æ–‡Tokenï¼šè¿”å›å¥å­ç¼–å·ï¼ˆä»0å¼€å§‹ç¼–å·ï¼‰ã€‚\n",
    "\n",
    "ç»¼ä¸Šï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥å¾ˆæ–¹ä¾¿çš„åœ¨ä¸€ä¸ªè¾“å…¥ç‰¹å¾ä¸­æ‰¾åˆ°ç­”æ¡ˆçš„èµ·å§‹å’Œç»“æŸ Tokenã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    }
   ],
   "source": [
    "sequence_ids = tokenized_example.sequence_ids()\n",
    "print(sequence_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 26\n"
     ]
    }
   ],
   "source": [
    "answers = example[\"answers\"]\n",
    "start_char = answers[\"answer_start\"][0]\n",
    "end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "# å½“å‰spanåœ¨æ–‡æœ¬ä¸­çš„èµ·å§‹æ ‡è®°ç´¢å¼•ã€‚\n",
    "token_start_index = 0\n",
    "while sequence_ids[token_start_index] != 1:\n",
    "    token_start_index += 1\n",
    "\n",
    "# å½“å‰spanåœ¨æ–‡æœ¬ä¸­çš„ç»“æŸæ ‡è®°ç´¢å¼•ã€‚\n",
    "token_end_index = len(tokenized_example[\"input_ids\"][0]) - 1\n",
    "while sequence_ids[token_end_index] != 1:\n",
    "    token_end_index -= 1\n",
    "\n",
    "# æ£€æµ‹ç­”æ¡ˆæ˜¯å¦è¶…å‡ºspanèŒƒå›´ï¼ˆå¦‚æœè¶…å‡ºèŒƒå›´ï¼Œè¯¥ç‰¹å¾å°†ä»¥CLSæ ‡è®°ç´¢å¼•æ ‡è®°ï¼‰ã€‚\n",
    "offsets = tokenized_example[\"offset_mapping\"][0]\n",
    "if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "    # å°†token_start_indexå’Œtoken_end_indexç§»åŠ¨åˆ°ç­”æ¡ˆçš„ä¸¤ç«¯ã€‚\n",
    "    # æ³¨æ„ï¼šå¦‚æœç­”æ¡ˆæ˜¯æœ€åä¸€ä¸ªå•è¯ï¼Œæˆ‘ä»¬å¯ä»¥ç§»åˆ°æœ€åä¸€ä¸ªæ ‡è®°ä¹‹åï¼ˆè¾¹ç•Œæƒ…å†µï¼‰ã€‚\n",
    "    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "        token_start_index += 1\n",
    "    start_position = token_start_index - 1\n",
    "    while offsets[token_end_index][1] >= end_char:\n",
    "        token_end_index -= 1\n",
    "    end_position = token_end_index + 1\n",
    "    print(start_position, end_position)\n",
    "else:\n",
    "    print(\"ç­”æ¡ˆä¸åœ¨æ­¤ç‰¹å¾ä¸­ã€‚\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‰“å°æ£€æŸ¥æ˜¯å¦å‡†ç¡®æ‰¾åˆ°äº†èµ·å§‹ä½ç½®ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over 1, 600\n",
      "over 1,600\n"
     ]
    }
   ],
   "source": [
    "# é€šè¿‡æŸ¥æ‰¾ offset mapping ä½ç½®ï¼Œè§£ç  context ä¸­çš„ç­”æ¡ˆ \n",
    "print(tokenizer.decode(tokenized_example[\"input_ids\"][0][start_position: end_position+1]))\n",
    "# ç›´æ¥æ‰“å° æ•°æ®é›†ä¸­çš„æ ‡å‡†ç­”æ¡ˆï¼ˆanswer[\"text\"])\n",
    "print(answers[\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å…³äºå¡«å……çš„ç­–ç•¥\n",
    "\n",
    "- å¯¹äºæ²¡æœ‰è¶…è¿‡æœ€å¤§é•¿åº¦çš„æ–‡æœ¬ï¼Œå¡«å……è¡¥é½é•¿åº¦ã€‚\n",
    "- å¯¹äºéœ€è¦å·¦ä¾§å¡«å……çš„æ¨¡å‹ï¼Œäº¤æ¢ question å’Œ context é¡ºåº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_on_right = tokenizer.padding_side == \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ•´åˆä»¥ä¸Šæ‰€æœ‰é¢„å¤„ç†æ­¥éª¤\n",
    "\n",
    "è®©æˆ‘ä»¬å°†æ‰€æœ‰å†…å®¹æ•´åˆåˆ°ä¸€ä¸ªå‡½æ•°ä¸­ï¼Œå¹¶å°†å…¶åº”ç”¨åˆ°è®­ç»ƒé›†ã€‚\n",
    "\n",
    "é’ˆå¯¹ä¸å¯å›ç­”çš„æƒ…å†µï¼ˆä¸Šä¸‹æ–‡è¿‡é•¿ï¼Œç­”æ¡ˆåœ¨å¦ä¸€ä¸ªç‰¹å¾ä¸­ï¼‰ï¼Œæˆ‘ä»¬ä¸ºå¼€å§‹å’Œç»“æŸä½ç½®éƒ½è®¾ç½®äº†clsç´¢å¼•ã€‚\n",
    "\n",
    "å¦‚æœallow_impossible_answersæ ‡å¿—ä¸ºFalseï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ç®€å•åœ°ä»è®­ç»ƒé›†ä¸­ä¸¢å¼ƒè¿™äº›ç¤ºä¾‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    # ä¸€äº›é—®é¢˜çš„å·¦ä¾§å¯èƒ½æœ‰å¾ˆå¤šç©ºç™½å­—ç¬¦ï¼Œè¿™å¯¹æˆ‘ä»¬æ²¡æœ‰ç”¨ï¼Œè€Œä¸”ä¼šå¯¼è‡´ä¸Šä¸‹æ–‡çš„æˆªæ–­å¤±è´¥\n",
    "    # ï¼ˆæ ‡è®°åŒ–çš„é—®é¢˜å°†å ç”¨å¤§é‡ç©ºé—´ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åˆ é™¤å·¦ä¾§çš„ç©ºç™½å­—ç¬¦ã€‚\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # ä½¿ç”¨æˆªæ–­å’Œå¡«å……å¯¹æˆ‘ä»¬çš„ç¤ºä¾‹è¿›è¡Œæ ‡è®°åŒ–ï¼Œä½†ä¿ç•™æº¢å‡ºéƒ¨åˆ†ï¼Œä½¿ç”¨æ­¥å¹…ï¼ˆstrideï¼‰ã€‚\n",
    "    # å½“ä¸Šä¸‹æ–‡å¾ˆé•¿æ—¶ï¼Œè¿™ä¼šå¯¼è‡´ä¸€ä¸ªç¤ºä¾‹å¯èƒ½æä¾›å¤šä¸ªç‰¹å¾ï¼Œå…¶ä¸­æ¯ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡éƒ½ä¸å‰ä¸€ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡æœ‰ä¸€äº›é‡å ã€‚\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # ç”±äºä¸€ä¸ªç¤ºä¾‹å¯èƒ½ç»™æˆ‘ä»¬æä¾›å¤šä¸ªç‰¹å¾ï¼ˆå¦‚æœå®ƒå…·æœ‰å¾ˆé•¿çš„ä¸Šä¸‹æ–‡ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä»ç‰¹å¾åˆ°å…¶å¯¹åº”ç¤ºä¾‹çš„æ˜ å°„ã€‚è¿™ä¸ªé”®å°±æä¾›äº†è¿™ä¸ªæ˜ å°„å…³ç³»ã€‚\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # åç§»æ˜ å°„å°†ä¸ºæˆ‘ä»¬æä¾›ä»ä»¤ç‰Œåˆ°åŸå§‹ä¸Šä¸‹æ–‡ä¸­çš„å­—ç¬¦ä½ç½®çš„æ˜ å°„ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬è®¡ç®—å¼€å§‹ä½ç½®å’Œç»“æŸä½ç½®ã€‚\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # è®©æˆ‘ä»¬ä¸ºè¿™äº›ç¤ºä¾‹è¿›è¡Œæ ‡è®°ï¼\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # æˆ‘ä»¬å°†ä½¿ç”¨ CLS ç‰¹æ®Š token çš„ç´¢å¼•æ¥æ ‡è®°ä¸å¯èƒ½çš„ç­”æ¡ˆã€‚\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # è·å–ä¸è¯¥ç¤ºä¾‹å¯¹åº”çš„åºåˆ—ï¼ˆä»¥äº†è§£ä¸Šä¸‹æ–‡å’Œé—®é¢˜æ˜¯ä»€ä¹ˆï¼‰ã€‚\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # ä¸€ä¸ªç¤ºä¾‹å¯ä»¥æä¾›å¤šä¸ªè·¨åº¦ï¼Œè¿™æ˜¯åŒ…å«æ­¤æ–‡æœ¬è·¨åº¦çš„ç¤ºä¾‹çš„ç´¢å¼•ã€‚\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # å¦‚æœæ²¡æœ‰ç»™å‡ºç­”æ¡ˆï¼Œåˆ™å°†cls_indexè®¾ç½®ä¸ºç­”æ¡ˆã€‚\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # ç­”æ¡ˆåœ¨æ–‡æœ¬ä¸­çš„å¼€å§‹å’Œç»“æŸå­—ç¬¦ç´¢å¼•ã€‚\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # å½“å‰è·¨åº¦åœ¨æ–‡æœ¬ä¸­çš„å¼€å§‹ä»¤ç‰Œç´¢å¼•ã€‚\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # å½“å‰è·¨åº¦åœ¨æ–‡æœ¬ä¸­çš„ç»“æŸä»¤ç‰Œç´¢å¼•ã€‚\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # æ£€æµ‹ç­”æ¡ˆæ˜¯å¦è¶…å‡ºè·¨åº¦ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¯¥ç‰¹å¾çš„æ ‡ç­¾å°†ä½¿ç”¨CLSç´¢å¼•ï¼‰ã€‚\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # å¦åˆ™ï¼Œå°†token_start_indexå’Œtoken_end_indexç§»åˆ°ç­”æ¡ˆçš„ä¸¤ç«¯ã€‚\n",
    "                # æ³¨æ„ï¼šå¦‚æœç­”æ¡ˆæ˜¯æœ€åä¸€ä¸ªå•è¯ï¼ˆè¾¹ç¼˜æƒ…å†µï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æœ€åä¸€ä¸ªåç§»ä¹‹åç»§ç»­ã€‚\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS-6iXTkIrJT"
   },
   "source": [
    "#### datasets.map çš„è¿›é˜¶ä½¿ç”¨\n",
    "\n",
    "ä½¿ç”¨ `datasets.map` æ–¹æ³•å°† `prepare_train_features` åº”ç”¨äºæ‰€æœ‰è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®ï¼š\n",
    "\n",
    "- batched: æ‰¹é‡å¤„ç†æ•°æ®ã€‚\n",
    "- remove_columns: å› ä¸ºé¢„å¤„ç†æ›´æ”¹äº†æ ·æœ¬çš„æ•°é‡ï¼Œæ‰€ä»¥åœ¨åº”ç”¨å®ƒæ—¶éœ€è¦åˆ é™¤æ—§åˆ—ã€‚\n",
    "- load_from_cache_fileï¼šæ˜¯å¦ä½¿ç”¨datasetsåº“çš„è‡ªåŠ¨ç¼“å­˜\n",
    "\n",
    "datasets åº“é’ˆå¯¹å¤§è§„æ¨¡æ•°æ®ï¼Œå®ç°äº†é«˜æ•ˆç¼“å­˜æœºåˆ¶ï¼Œèƒ½å¤Ÿè‡ªåŠ¨æ£€æµ‹ä¼ é€’ç»™ map çš„å‡½æ•°æ˜¯å¦å·²æ›´æ”¹ï¼ˆå› æ­¤éœ€è¦ä¸ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼‰ã€‚å¦‚æœåœ¨è°ƒç”¨ map æ—¶è®¾ç½® `load_from_cache_file=False`ï¼Œå¯ä»¥å¼ºåˆ¶é‡æ–°åº”ç”¨é¢„å¤„ç†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "DDtsaJeVIrJT",
    "outputId": "aa4734bf-4ef5-4437-9948-2c16363da719"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87599/87599 [00:34<00:00, 2505.38 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10570/10570 [00:04<00:00, 2408.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(prepare_train_features,\n",
    "                                  batched=True,\n",
    "                                  remove_columns=datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## å¾®è°ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBiW8UpKIrJW"
   },
   "source": [
    "ç°åœ¨æˆ‘ä»¬çš„æ•°æ®å·²ç»å‡†å¤‡å¥½ç”¨äºè®­ç»ƒï¼Œæˆ‘ä»¬å¯ä»¥ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶è¿›è¡Œå¾®è°ƒã€‚\n",
    "\n",
    "ç”±äºæˆ‘ä»¬çš„ä»»åŠ¡æ˜¯é—®ç­”ï¼Œæˆ‘ä»¬ä½¿ç”¨ `AutoModelForQuestionAnswering` ç±»ã€‚(å¯¹æ¯” Yelp è¯„è®ºæ‰“åˆ†ä½¿ç”¨çš„æ˜¯ `AutoModelForSequenceClassification` ç±»ï¼‰\n",
    "\n",
    "è­¦å‘Šé€šçŸ¥æˆ‘ä»¬æ­£åœ¨ä¸¢å¼ƒä¸€äº›æƒé‡ï¼ˆ`vocab_transform` å’Œ `vocab_layer_norm` å±‚ï¼‰ï¼Œå¹¶éšæœºåˆå§‹åŒ–å…¶ä»–ä¸€äº›æƒé‡ï¼ˆ`pre_classifier` å’Œ `classifier` å±‚ï¼‰ã€‚åœ¨å¾®è°ƒæ¨¡å‹æƒ…å†µä¸‹æ˜¯ç»å¯¹æ­£å¸¸çš„ï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨åˆ é™¤ç”¨äºé¢„è®­ç»ƒæ¨¡å‹çš„æ©ç è¯­è¨€å»ºæ¨¡ä»»åŠ¡çš„å¤´éƒ¨ï¼Œå¹¶ç”¨ä¸€ä¸ªæ–°çš„å¤´éƒ¨æ›¿æ¢å®ƒï¼Œå¯¹äºè¿™ä¸ªæ–°å¤´éƒ¨ï¼Œæˆ‘ä»¬æ²¡æœ‰é¢„è®­ç»ƒçš„æƒé‡ï¼Œæ‰€ä»¥åº“ä¼šè­¦å‘Šæˆ‘ä»¬åœ¨ç”¨å®ƒè¿›è¡Œæ¨ç†ä¹‹å‰åº”è¯¥å¯¹è¿™ä¸ªæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè€Œè¿™æ­£æ˜¯æˆ‘ä»¬è¦åšçš„äº‹æƒ…ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "TlqNaB8jIrJW",
    "outputId": "84916cf3-6e6c-47f3-d081-032ec30a4132"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N8urzhyIrJY"
   },
   "source": [
    "#### è®­ç»ƒè¶…å‚æ•°ï¼ˆTrainingArgumentsï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "Bliy8zgjIrJY"
   },
   "outputs": [],
   "source": [
    "batch_size=5\n",
    "model_dir = f\"models/{model_checkpoint}-finetuned-squad\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,  \n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=4,\n",
    "    logging_steps=100, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collatorï¼ˆæ•°æ®æ•´ç†å™¨ï¼‰\n",
    "\n",
    "æ•°æ®æ•´ç†å™¨å°†è®­ç»ƒæ•°æ®æ•´ç†ä¸ºæ‰¹æ¬¡æ•°æ®ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒæ—¶çš„æ‰¹æ¬¡å¤„ç†ã€‚æœ¬æ•™ç¨‹ä½¿ç”¨é»˜è®¤çš„ `default_data_collator`ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXuFTAzDIrJe"
   },
   "source": [
    "### å®ä¾‹åŒ–è®­ç»ƒå™¨ï¼ˆTrainerï¼‰\n",
    "\n",
    "ä¸ºäº†å‡å°‘è®­ç»ƒæ—¶é—´ï¼ˆéœ€è¦å¤§é‡ç®—åŠ›æ”¯æŒï¼‰ï¼Œæˆ‘ä»¬ä¸åœ¨æœ¬æ•™ç¨‹çš„è®­ç»ƒæ¨¡å‹è¿‡ç¨‹ä¸­è®¡ç®—æ¨¡å‹è¯„ä¼°æŒ‡æ ‡ã€‚\n",
    "\n",
    "è€Œæ˜¯è®­ç»ƒå®Œæˆåï¼Œå†ç‹¬ç«‹è¿›è¡Œæ¨¡å‹è¯„ä¼°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "imY1oC3SIrJf"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU ä½¿ç”¨æƒ…å†µ\n",
    "\n",
    "è®­ç»ƒæ•°æ®ä¸æ¨¡å‹é…ç½®ï¼š\n",
    "\n",
    "- SQUAD v1.1\n",
    "- model_checkpoint = \"distilbert-base-uncased\"\n",
    "- batch_size = 64\n",
    "\n",
    "NVIDIA GPU ä½¿ç”¨æƒ…å†µï¼š\n",
    "\n",
    "```shell\n",
    "Every 1.0s: nvidia-smi                                                   Wed Dec 20 15:39:57 2023\n",
    "\n",
    "Wed Dec 20 15:39:57 2023\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  Tesla T4                       Off | 00000000:00:0D.0 Off |                    0 |\n",
    "| N/A   67C    P0              67W /  70W |  14617MiB / 15360MiB |    100%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     16384      C   /root/miniconda3/bin/python               14612MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug 23 21:21:22 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       On  |   00000000:00:07.0 Off |                    0 |\n",
      "| N/A   48C    P0             27W /   70W |   15089MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      3745      C   /usr/bin/python3                            11814MiB |\n",
      "|    0   N/A  N/A      3748      C   /usr/bin/python3                             3272MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "uNx5pyRlIrJh",
    "outputId": "077e661e-d36c-469b-89b8-7ff7f73541ec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53115' max='53115' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53115/53115 1:21:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.145100</td>\n",
       "      <td>1.162570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.856500</td>\n",
       "      <td>1.172747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.537200</td>\n",
       "      <td>1.323046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=53115, training_loss=0.9796613318698169, metrics={'train_runtime': 4891.7771, 'train_samples_per_second': 54.289, 'train_steps_per_second': 10.858, 'total_flos': 2.602335381127373e+16, 'train_loss': 0.9796613318698169, 'epoch': 3.0})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è®­ç»ƒå®Œæˆåï¼Œç¬¬ä¸€æ—¶é—´ä¿å­˜æ¨¡å‹æƒé‡æ–‡ä»¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = trainer.save_model(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¨¡å‹è¯„ä¼°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**è¯„ä¼°æ¨¡å‹è¾“å‡ºéœ€è¦ä¸€äº›é¢å¤–çš„å¤„ç†ï¼šå°†æ¨¡å‹çš„é¢„æµ‹æ˜ å°„å›ä¸Šä¸‹æ–‡çš„éƒ¨åˆ†ã€‚**\n",
    "\n",
    "æ¨¡å‹ç›´æ¥è¾“å‡ºçš„æ˜¯é¢„æµ‹ç­”æ¡ˆçš„`èµ·å§‹ä½ç½®`å’Œ`ç»“æŸä½ç½®`çš„**logits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'start_logits', 'end_logits'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "for batch in trainer.get_eval_dataloader():\n",
    "    break\n",
    "batch = {k: v.to(trainer.args.device) for k, v in batch.items()}\n",
    "with torch.no_grad():\n",
    "    output = trainer.model(**batch)\n",
    "output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¨¡å‹çš„è¾“å‡ºæ˜¯ä¸€ä¸ªç±»ä¼¼å­—å…¸çš„å¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«æŸå¤±ï¼ˆå› ä¸ºæˆ‘ä»¬æä¾›äº†æ ‡ç­¾ï¼‰ï¼Œä»¥åŠèµ·å§‹å’Œç»“æŸlogitsã€‚æˆ‘ä»¬ä¸éœ€è¦æŸå¤±æ¥è¿›è¡Œé¢„æµ‹ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹logitsï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 384]), torch.Size([5, 384]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.shape, output.end_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 46,  57,  78,  43, 118], device='cuda:0'),\n",
       " tensor([ 47,  58,  81,  44, 118], device='cuda:0'))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å¦‚ä½•ä»æ¨¡å‹è¾“å‡ºçš„ä½ç½® logit ç»„åˆæˆç­”æ¡ˆ\n",
    "\n",
    "æˆ‘ä»¬æœ‰æ¯ä¸ªç‰¹å¾å’Œæ¯ä¸ªæ ‡è®°çš„logitã€‚åœ¨æ¯ä¸ªç‰¹å¾ä¸­ä¸ºæ¯ä¸ªæ ‡è®°é¢„æµ‹ç­”æ¡ˆæœ€æ˜æ˜¾çš„æ–¹æ³•æ˜¯ï¼Œå°†èµ·å§‹logitsçš„æœ€å¤§ç´¢å¼•ä½œä¸ºèµ·å§‹ä½ç½®ï¼Œå°†ç»“æŸlogitsçš„æœ€å¤§ç´¢å¼•ä½œä¸ºç»“æŸä½ç½®ã€‚\n",
    "\n",
    "åœ¨è®¸å¤šæƒ…å†µä¸‹è¿™ç§æ–¹å¼æ•ˆæœå¾ˆå¥½ï¼Œä½†æ˜¯å¦‚æœæ­¤é¢„æµ‹ç»™å‡ºäº†ä¸å¯èƒ½çš„ç»“æœè¯¥æ€ä¹ˆåŠï¼Ÿæ¯”å¦‚ï¼šèµ·å§‹ä½ç½®å¯èƒ½å¤§äºç»“æŸä½ç½®ï¼Œæˆ–è€…æŒ‡å‘é—®é¢˜ä¸­çš„æ–‡æœ¬ç‰‡æ®µè€Œä¸æ˜¯ç­”æ¡ˆã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›æŸ¥çœ‹ç¬¬äºŒå¥½çš„é¢„æµ‹ï¼Œçœ‹å®ƒæ˜¯å¦ç»™å‡ºäº†ä¸€ä¸ªå¯èƒ½çš„ç­”æ¡ˆï¼Œå¹¶é€‰æ‹©å®ƒã€‚\n",
    "\n",
    "é€‰æ‹©ç¬¬äºŒå¥½çš„ç­”æ¡ˆå¹¶ä¸åƒé€‰æ‹©æœ€ä½³ç­”æ¡ˆé‚£ä¹ˆå®¹æ˜“ï¼š\n",
    "- å®ƒæ˜¯èµ·å§‹logitsä¸­ç¬¬äºŒä½³ç´¢å¼•ä¸ç»“æŸlogitsä¸­æœ€ä½³ç´¢å¼•å—ï¼Ÿ\n",
    "- è¿˜æ˜¯èµ·å§‹logitsä¸­æœ€ä½³ç´¢å¼•ä¸ç»“æŸlogitsä¸­ç¬¬äºŒä½³ç´¢å¼•ï¼Ÿ\n",
    "- å¦‚æœç¬¬äºŒå¥½çš„ç­”æ¡ˆä¹Ÿä¸å¯èƒ½ï¼Œé‚£ä¹ˆå¯¹äºç¬¬ä¸‰å¥½çš„ç­”æ¡ˆï¼Œæƒ…å†µä¼šæ›´åŠ æ£˜æ‰‹ã€‚\n",
    "\n",
    "ä¸ºäº†å¯¹ç­”æ¡ˆè¿›è¡Œåˆ†ç±»ï¼Œ\n",
    "1. å°†ä½¿ç”¨é€šè¿‡æ·»åŠ èµ·å§‹å’Œç»“æŸlogitsè·å¾—çš„åˆ†æ•°\n",
    "1. è®¾è®¡ä¸€ä¸ªåä¸º`n_best_size`çš„è¶…å‚æ•°ï¼Œé™åˆ¶ä¸å¯¹æ‰€æœ‰å¯èƒ½çš„ç­”æ¡ˆè¿›è¡Œæ’åºã€‚\n",
    "1. æˆ‘ä»¬å°†é€‰æ‹©èµ·å§‹å’Œç»“æŸlogitsä¸­çš„æœ€ä½³ç´¢å¼•ï¼Œå¹¶æ”¶é›†è¿™äº›é¢„æµ‹çš„æ‰€æœ‰ç­”æ¡ˆã€‚\n",
    "1. åœ¨æ£€æŸ¥æ¯ä¸€ä¸ªæ˜¯å¦æœ‰æ•ˆåï¼Œæˆ‘ä»¬å°†æŒ‰ç…§å…¶åˆ†æ•°å¯¹å®ƒä»¬è¿›è¡Œæ’åºï¼Œå¹¶ä¿ç•™æœ€ä½³çš„ç­”æ¡ˆã€‚\n",
    "\n",
    "ä»¥ä¸‹æ˜¯æˆ‘ä»¬å¦‚ä½•åœ¨æ‰¹æ¬¡ä¸­çš„ç¬¬ä¸€ä¸ªç‰¹å¾ä¸Šæ‰§è¡Œæ­¤æ“ä½œçš„ç¤ºä¾‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_best_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "\n",
    "# è·å–æœ€ä½³çš„èµ·å§‹å’Œç»“æŸä½ç½®çš„ç´¢å¼•ï¼š\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "\n",
    "valid_answers = []\n",
    "\n",
    "# éå†èµ·å§‹ä½ç½®å’Œç»“æŸä½ç½®çš„ç´¢å¼•ç»„åˆ\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        if start_index <= end_index:  # éœ€è¦è¿›ä¸€æ­¥æµ‹è¯•ä»¥æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": \"\"  # æˆ‘ä»¬éœ€è¦æ‰¾åˆ°ä¸€ç§æ–¹æ³•æ¥è·å–ä¸ä¸Šä¸‹æ–‡ä¸­ç­”æ¡ˆå¯¹åº”çš„åŸå§‹å­å­—ç¬¦ä¸²\n",
    "                }\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®å®ƒä»¬çš„å¾—åˆ†å¯¹`valid_answers`è¿›è¡Œæ’åºï¼Œå¹¶ä»…ä¿ç•™æœ€ä½³ç­”æ¡ˆã€‚å”¯ä¸€å‰©ä¸‹çš„é—®é¢˜æ˜¯å¦‚ä½•æ£€æŸ¥ç»™å®šçš„è·¨åº¦æ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼ˆè€Œä¸æ˜¯é—®é¢˜ä¸­ï¼‰ï¼Œä»¥åŠå¦‚ä½•è·å–å…¶ä¸­çš„æ–‡æœ¬ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦å‘æˆ‘ä»¬çš„éªŒè¯ç‰¹å¾æ·»åŠ ä¸¤ä¸ªå†…å®¹ï¼š\n",
    "\n",
    "- ç”Ÿæˆè¯¥ç‰¹å¾çš„ç¤ºä¾‹çš„IDï¼ˆå› ä¸ºæ¯ä¸ªç¤ºä¾‹å¯ä»¥ç”Ÿæˆå¤šä¸ªç‰¹å¾ï¼Œå¦‚å‰æ‰€ç¤ºï¼‰ï¼›\n",
    "- åç§»æ˜ å°„ï¼Œå®ƒå°†ä¸ºæˆ‘ä»¬æä¾›ä»æ ‡è®°ç´¢å¼•åˆ°ä¸Šä¸‹æ–‡ä¸­å­—ç¬¦ä½ç½®çš„æ˜ å°„ã€‚\n",
    "\n",
    "è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹å‡½æ•°ç¨å¾®ä¸åŒäº`prepare_train_features`æ¥é‡æ–°å¤„ç†éªŒè¯é›†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    # ä¸€äº›é—®é¢˜çš„å·¦ä¾§æœ‰å¾ˆå¤šç©ºç™½ï¼Œè¿™äº›ç©ºç™½å¹¶ä¸æœ‰ç”¨ä¸”ä¼šå¯¼è‡´ä¸Šä¸‹æ–‡æˆªæ–­å¤±è´¥ï¼ˆåˆ†è¯åçš„é—®é¢˜ä¼šå ç”¨å¾ˆå¤šç©ºé—´ï¼‰ã€‚\n",
    "    # å› æ­¤æˆ‘ä»¬ç§»é™¤è¿™äº›å·¦ä¾§ç©ºç™½\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # ä½¿ç”¨æˆªæ–­å’Œå¯èƒ½çš„å¡«å……å¯¹æˆ‘ä»¬çš„ç¤ºä¾‹è¿›è¡Œåˆ†è¯ï¼Œä½†ä½¿ç”¨æ­¥é•¿ä¿ç•™æº¢å‡ºçš„ä»¤ç‰Œã€‚è¿™å¯¼è‡´ä¸€ä¸ªé•¿ä¸Šä¸‹æ–‡çš„ç¤ºä¾‹å¯èƒ½äº§ç”Ÿ\n",
    "    # å‡ ä¸ªç‰¹å¾ï¼Œæ¯ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡éƒ½ä¼šç¨å¾®ä¸å‰ä¸€ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡é‡å ã€‚\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # ç”±äºä¸€ä¸ªç¤ºä¾‹åœ¨ä¸Šä¸‹æ–‡å¾ˆé•¿æ—¶å¯èƒ½ä¼šäº§ç”Ÿå‡ ä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä»ç‰¹å¾æ˜ å°„åˆ°å…¶å¯¹åº”ç¤ºä¾‹çš„æ˜ å°„ã€‚è¿™ä¸ªé”®å°±æ˜¯ä¸ºäº†è¿™ä¸ªç›®çš„ã€‚\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # æˆ‘ä»¬ä¿ç•™äº§ç”Ÿè¿™ä¸ªç‰¹å¾çš„ç¤ºä¾‹IDï¼Œå¹¶ä¸”ä¼šå­˜å‚¨åç§»æ˜ å°„ã€‚\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # è·å–ä¸è¯¥ç¤ºä¾‹å¯¹åº”çš„åºåˆ—ï¼ˆä»¥äº†è§£å“ªäº›æ˜¯ä¸Šä¸‹æ–‡ï¼Œå“ªäº›æ˜¯é—®é¢˜ï¼‰ã€‚\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # ä¸€ä¸ªç¤ºä¾‹å¯ä»¥äº§ç”Ÿå‡ ä¸ªæ–‡æœ¬æ®µï¼Œè¿™é‡Œæ˜¯åŒ…å«è¯¥æ–‡æœ¬æ®µçš„ç¤ºä¾‹çš„ç´¢å¼•ã€‚\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # å°†ä¸å±äºä¸Šä¸‹æ–‡çš„åç§»æ˜ å°„è®¾ç½®ä¸ºNoneï¼Œä»¥ä¾¿å®¹æ˜“ç¡®å®šä¸€ä¸ªä»¤ç‰Œä½ç½®æ˜¯å¦å±äºä¸Šä¸‹æ–‡ã€‚\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å°†`prepare_validation_features`åº”ç”¨åˆ°æ•´ä¸ªéªŒè¯é›†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10570/10570 [00:05<00:00, 1945.11 examples/s]\n"
     ]
    }
   ],
   "source": [
    "validation_features = datasets[\"validation\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"validation\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can grab the predictions for all features by using the `Trainer.predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "raw_predictions = trainer.predict(validation_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Trainer`ä¼šéšè—æ¨¡å‹ä¸ä½¿ç”¨çš„åˆ—ï¼ˆåœ¨è¿™é‡Œæ˜¯`example_id`å’Œ`offset_mapping`ï¼Œæˆ‘ä»¬éœ€è¦å®ƒä»¬è¿›è¡Œåå¤„ç†ï¼‰ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å°†å®ƒä»¬é‡æ–°è®¾ç½®å›æ¥ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥æ”¹è¿›ä¹‹å‰çš„æµ‹è¯•ï¼š\n",
    "\n",
    "ç”±äºåœ¨åç§»æ˜ å°„ä¸­ï¼Œå½“å®ƒå¯¹åº”äºé—®é¢˜çš„ä¸€éƒ¨åˆ†æ—¶ï¼Œæˆ‘ä»¬å°†å…¶è®¾ç½®ä¸ºNoneï¼Œå› æ­¤å¯ä»¥è½»æ¾æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦å®Œå…¨åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚æˆ‘ä»¬è¿˜å¯ä»¥ä»è€ƒè™‘ä¸­æ’é™¤éå¸¸é•¿çš„ç­”æ¡ˆï¼ˆå¯ä»¥è°ƒæ•´çš„è¶…å‚æ•°ï¼‰ã€‚\n",
    "\n",
    "å±•å¼€è¯´ä¸‹å…·ä½“å®ç°ï¼š\n",
    "- é¦–å…ˆä»æ¨¡å‹è¾“å‡ºä¸­è·å–èµ·å§‹å’Œç»“æŸçš„é€»è¾‘å€¼ï¼ˆlogitsï¼‰ï¼Œè¿™äº›å€¼è¡¨æ˜ç­”æ¡ˆåœ¨æ–‡æœ¬ä¸­å¯èƒ½å¼€å§‹å’Œç»“æŸçš„ä½ç½®ã€‚\n",
    "- ç„¶åï¼Œå®ƒä½¿ç”¨åç§»æ˜ å°„ï¼ˆoffset_mappingï¼‰æ¥æ‰¾åˆ°è¿™äº›é€»è¾‘å€¼åœ¨åŸå§‹æ–‡æœ¬ä¸­çš„å…·ä½“ä½ç½®ã€‚\n",
    "- æ¥ä¸‹æ¥ï¼Œä»£ç éå†å¯èƒ½çš„å¼€å§‹å’Œç»“æŸç´¢å¼•ç»„åˆï¼Œæ’é™¤é‚£äº›ä¸åœ¨ä¸Šä¸‹æ–‡èŒƒå›´å†…æˆ–é•¿åº¦ä¸åˆé€‚çš„ç­”æ¡ˆã€‚\n",
    "- å¯¹äºæœ‰æ•ˆçš„ç­”æ¡ˆï¼Œå®ƒè®¡ç®—å‡ºä¸€ä¸ªåˆ†æ•°ï¼ˆåŸºäºå¼€å§‹å’Œç»“æŸé€»è¾‘å€¼çš„å’Œï¼‰ï¼Œå¹¶å°†ç­”æ¡ˆåŠå…¶åˆ†æ•°å­˜å‚¨èµ·æ¥ã€‚\n",
    "- æœ€åï¼Œå®ƒæ ¹æ®åˆ†æ•°å¯¹ç­”æ¡ˆè¿›è¡Œæ’åºï¼Œå¹¶è¿”å›å¾—åˆ†æœ€é«˜çš„å‡ ä¸ªç­”æ¡ˆã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_answer_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 22.960938, 'text': 'Denver Broncos'},\n",
       " {'score': 20.492188,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 17.820312, 'text': 'Broncos'},\n",
       " {'score': 17.273438, 'text': 'Carolina Panthers'},\n",
       " {'score': 15.3515625,\n",
       "  'text': 'Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 14.810547, 'text': 'Denver'},\n",
       " {'score': 12.949219,\n",
       "  'text': 'The American Football Conference (AFC) champion Denver Broncos'},\n",
       " {'score': 11.240234, 'text': 'Panthers'},\n",
       " {'score': 11.172241,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title.'},\n",
       " {'score': 10.971924,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10'},\n",
       " {'score': 10.919922,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina'},\n",
       " {'score': 10.837891,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC)'},\n",
       " {'score': 10.480469,\n",
       "  'text': 'The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 10.3203125,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC'},\n",
       " {'score': 9.566406,\n",
       "  'text': 'American Football Conference (AFC) champion Denver Broncos'},\n",
       " {'score': 9.1875, 'text': 'champion Denver Broncos'},\n",
       " {'score': 9.015625,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference'},\n",
       " {'score': 8.451172,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion'},\n",
       " {'score': 7.953491,\n",
       "  'text': 'Carolina Panthers 24â€“10 to earn their third Super Bowl title.'},\n",
       " {'score': 7.753174, 'text': 'Carolina Panthers 24â€“10'}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "offset_mapping = validation_features[0][\"offset_mapping\"]\n",
    "\n",
    "# ç¬¬ä¸€ä¸ªç‰¹å¾æ¥è‡ªç¬¬ä¸€ä¸ªç¤ºä¾‹ã€‚å¯¹äºæ›´ä¸€èˆ¬çš„æƒ…å†µï¼Œæˆ‘ä»¬éœ€è¦å°†example_idåŒ¹é…åˆ°ä¸€ä¸ªç¤ºä¾‹ç´¢å¼•\n",
    "context = datasets[\"validation\"][0][\"context\"]\n",
    "\n",
    "# æ”¶é›†æœ€ä½³å¼€å§‹/ç»“æŸé€»è¾‘çš„ç´¢å¼•ï¼š\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "valid_answers = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        # ä¸è€ƒè™‘è¶…å‡ºèŒƒå›´çš„ç­”æ¡ˆï¼ŒåŸå› æ˜¯ç´¢å¼•è¶…å‡ºèŒƒå›´æˆ–å¯¹åº”äºè¾“å…¥IDçš„éƒ¨åˆ†ä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚\n",
    "        if (\n",
    "            start_index >= len(offset_mapping)\n",
    "            or end_index >= len(offset_mapping)\n",
    "            or offset_mapping[start_index] is None\n",
    "            or offset_mapping[end_index] is None\n",
    "        ):\n",
    "            continue\n",
    "        # ä¸è€ƒè™‘é•¿åº¦å°äº0æˆ–å¤§äºmax_answer_lengthçš„ç­”æ¡ˆã€‚\n",
    "        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "            continue\n",
    "        if start_index <= end_index: # æˆ‘ä»¬éœ€è¦ç»†åŒ–è¿™ä¸ªæµ‹è¯•ï¼Œä»¥æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­\n",
    "            start_char = offset_mapping[start_index][0]\n",
    "            end_char = offset_mapping[end_index][1]\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": context[start_char: end_char]\n",
    "                }\n",
    "            )\n",
    "\n",
    "valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
    "valid_answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‰“å°æ¯”è¾ƒæ¨¡å‹è¾“å‡ºå’Œæ ‡å‡†ç­”æ¡ˆï¼ˆGround-truthï¼‰æ˜¯å¦ä¸€è‡´:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
       " 'answer_start': [177, 177, 177]}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"validation\"][0][\"answers\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**æ¨¡å‹æœ€é«˜æ¦‚ç‡çš„è¾“å‡ºä¸æ ‡å‡†ç­”æ¡ˆä¸€è‡´**\n",
    "\n",
    "æ­£å¦‚ä¸Šé¢çš„ä»£ç æ‰€ç¤ºï¼Œè¿™åœ¨ç¬¬ä¸€ä¸ªç‰¹å¾ä¸Šå¾ˆå®¹æ˜“ï¼Œå› ä¸ºæˆ‘ä»¬çŸ¥é“å®ƒæ¥è‡ªç¬¬ä¸€ä¸ªç¤ºä¾‹ã€‚\n",
    "\n",
    "å¯¹äºå…¶ä»–ç‰¹å¾ï¼Œæˆ‘ä»¬éœ€è¦å»ºç«‹ä¸€ä¸ªç¤ºä¾‹ä¸å…¶å¯¹åº”ç‰¹å¾çš„æ˜ å°„å…³ç³»ã€‚\n",
    "\n",
    "æ­¤å¤–ï¼Œç”±äºä¸€ä¸ªç¤ºä¾‹å¯ä»¥ç”Ÿæˆå¤šä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬éœ€è¦å°†ç”±ç»™å®šç¤ºä¾‹ç”Ÿæˆçš„æ‰€æœ‰ç‰¹å¾ä¸­çš„æ‰€æœ‰ç­”æ¡ˆæ±‡é›†åœ¨ä¸€èµ·ï¼Œç„¶åé€‰æ‹©æœ€ä½³ç­”æ¡ˆã€‚\n",
    "\n",
    "ä¸‹é¢çš„ä»£ç æ„å»ºäº†ä¸€ä¸ªç¤ºä¾‹ç´¢å¼•åˆ°å…¶å¯¹åº”ç‰¹å¾ç´¢å¼•çš„æ˜ å°„å…³ç³»ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "examples = datasets[\"validation\"]\n",
    "features = validation_features\n",
    "\n",
    "example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "features_per_example = collections.defaultdict(list)\n",
    "for i, feature in enumerate(features):\n",
    "    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å½“`squad_v2 = True`æ—¶ï¼Œæœ‰ä¸€å®šæ¦‚ç‡å‡ºç°ä¸å¯èƒ½çš„ç­”æ¡ˆï¼ˆimpossible answer)ã€‚\n",
    "\n",
    "ä¸Šé¢çš„ä»£ç ä»…ä¿ç•™åœ¨ä¸Šä¸‹æ–‡ä¸­çš„ç­”æ¡ˆï¼Œæˆ‘ä»¬è¿˜éœ€è¦è·å–ä¸å¯èƒ½ç­”æ¡ˆçš„åˆ†æ•°ï¼ˆå…¶èµ·å§‹å’Œç»“æŸç´¢å¼•å¯¹åº”äºCLSæ ‡è®°çš„ç´¢å¼•ï¼‰ã€‚\n",
    "\n",
    "å½“ä¸€ä¸ªç¤ºä¾‹ç”Ÿæˆå¤šä¸ªç‰¹å¾æ—¶ï¼Œæˆ‘ä»¬å¿…é¡»åœ¨æ‰€æœ‰ç‰¹å¾ä¸­çš„ä¸å¯èƒ½ç­”æ¡ˆéƒ½é¢„æµ‹å‡ºç°ä¸å¯èƒ½ç­”æ¡ˆæ—¶ï¼ˆå› ä¸ºä¸€ä¸ªç‰¹å¾å¯èƒ½ä¹‹æ‰€ä»¥èƒ½å¤Ÿé¢„æµ‹å‡ºä¸å¯èƒ½ç­”æ¡ˆï¼Œæ˜¯å› ä¸ºç­”æ¡ˆä¸åœ¨å®ƒå¯ä»¥è®¿é—®çš„ä¸Šä¸‹æ–‡éƒ¨åˆ†ï¼‰ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆä¸€ä¸ªç¤ºä¾‹ä¸­ä¸å¯èƒ½ç­”æ¡ˆçš„åˆ†æ•°æ˜¯è¯¥ç¤ºä¾‹ç”Ÿæˆçš„æ¯ä¸ªç‰¹å¾ä¸­çš„ä¸å¯èƒ½ç­”æ¡ˆçš„åˆ†æ•°çš„æœ€å°å€¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    # æ„å»ºä¸€ä¸ªä»ç¤ºä¾‹åˆ°å…¶å¯¹åº”ç‰¹å¾çš„æ˜ å°„ã€‚\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # æˆ‘ä»¬éœ€è¦å¡«å……çš„å­—å…¸ã€‚\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    # æ—¥å¿—è®°å½•ã€‚\n",
    "    print(f\"æ­£åœ¨åå¤„ç† {len(examples)} ä¸ªç¤ºä¾‹çš„é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹åˆ†æ•£åœ¨ {len(features)} ä¸ªç‰¹å¾ä¸­ã€‚\")\n",
    "\n",
    "    # éå†æ‰€æœ‰ç¤ºä¾‹ï¼\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # è¿™äº›æ˜¯ä¸å½“å‰ç¤ºä¾‹å…³è”çš„ç‰¹å¾çš„ç´¢å¼•ã€‚\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_score = None # ä»…åœ¨squad_v2ä¸ºTrueæ—¶ä½¿ç”¨ã€‚\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        # éå†ä¸å½“å‰ç¤ºä¾‹å…³è”çš„æ‰€æœ‰ç‰¹å¾ã€‚\n",
    "        for feature_index in feature_indices:\n",
    "            # æˆ‘ä»¬è·å–æ¨¡å‹å¯¹è¿™ä¸ªç‰¹å¾çš„é¢„æµ‹ã€‚\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # è¿™å°†å…è®¸æˆ‘ä»¬å°†logitsä¸­çš„æŸäº›ä½ç½®æ˜ å°„åˆ°åŸå§‹ä¸Šä¸‹æ–‡ä¸­çš„æ–‡æœ¬è·¨åº¦ã€‚\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # æ›´æ–°æœ€å°ç©ºé¢„æµ‹ã€‚\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            # æµè§ˆæ‰€æœ‰çš„æœ€ä½³å¼€å§‹å’Œç»“æŸlogitsï¼Œä¸º `n_best_size` ä¸ªæœ€ä½³é€‰æ‹©ã€‚\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # ä¸è€ƒè™‘è¶…å‡ºèŒƒå›´çš„ç­”æ¡ˆï¼ŒåŸå› æ˜¯ç´¢å¼•è¶…å‡ºèŒƒå›´æˆ–å¯¹åº”äºè¾“å…¥IDçš„éƒ¨åˆ†ä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # ä¸è€ƒè™‘é•¿åº¦å°äº0æˆ–å¤§äºmax_answer_lengthçš„ç­”æ¡ˆã€‚\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            # åœ¨æå°‘æ•°æƒ…å†µä¸‹æˆ‘ä»¬æ²¡æœ‰ä¸€ä¸ªéç©ºé¢„æµ‹ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå‡é¢„æµ‹ä»¥é¿å…å¤±è´¥ã€‚\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        # é€‰æ‹©æˆ‘ä»¬çš„æœ€ç»ˆç­”æ¡ˆï¼šæœ€ä½³ç­”æ¡ˆæˆ–ç©ºç­”æ¡ˆï¼ˆä»…é€‚ç”¨äºsquad_v2ï¼‰\n",
    "        if not squad_v2:\n",
    "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "        else:\n",
    "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "            predictions[example[\"id\"]] = answer\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨åŸå§‹ç»“æœä¸Šåº”ç”¨åå¤„ç†é—®ç­”ç»“æœï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åå¤„ç† 10570 ä¸ªç¤ºä¾‹çš„é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹åˆ†æ•£åœ¨ 10784 ä¸ªç‰¹å¾ä¸­ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10570/10570 [00:28<00:00, 375.51it/s]\n"
     ]
    }
   ],
   "source": [
    "final_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, raw_predictions.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨ `datasets.load_metric` ä¸­åŠ è½½ `SQuAD v2` çš„è¯„ä¼°æŒ‡æ ‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.cloud.aliyuncs.com/pypi/simple/\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.10.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.1.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.34.4)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.5)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.16.1)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (21.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.19.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.12.15)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets>=2.0.0->evaluate) (5.4.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate) (2020.6.20)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate) (1.26.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate) (3.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->evaluate) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.6.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"squad_v2\" if squad_v2 else \"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯ä»¥è°ƒç”¨ä¸Šé¢å®šä¹‰çš„å‡½æ•°è¿›è¡Œè¯„ä¼°ã€‚\n",
    "\n",
    "åªéœ€ç¨å¾®è°ƒæ•´ä¸€ä¸‹é¢„æµ‹å’Œæ ‡ç­¾çš„æ ¼å¼ï¼Œå› ä¸ºå®ƒæœŸæœ›çš„æ˜¯ä¸€ç³»åˆ—å­—å…¸è€Œä¸æ˜¯ä¸€ä¸ªå¤§å­—å…¸ã€‚\n",
    "\n",
    "åœ¨ä½¿ç”¨`squad_v2`æ•°æ®é›†æ—¶ï¼Œæˆ‘ä»¬è¿˜éœ€è¦è®¾ç½®`no_answer_probability`å‚æ•°ï¼ˆæˆ‘ä»¬åœ¨è¿™é‡Œå°†å…¶è®¾ç½®ä¸º0.0ï¼Œå› ä¸ºå¦‚æœæˆ‘ä»¬é€‰æ‹©äº†ç­”æ¡ˆï¼Œæˆ‘ä»¬å·²ç»å°†ç­”æ¡ˆè®¾ç½®ä¸ºç©ºï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 77.60643330179754, 'f1': 85.72485909876409}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if squad_v2:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\n",
    "else:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
    "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n",
    "metric.compute(predictions=formatted_predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact_match': 77.60643330179754, 'f1': 85.72485909876409}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"squad_v2\" if squad_v2 else \"squad\")\n",
    "\n",
    "if squad_v2:\n",
    "    formatted_predictions = [\n",
    "        {\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} \n",
    "        for k, v in final_predictions.items()\n",
    "    ]\n",
    "else:\n",
    "    formatted_predictions = [\n",
    "        {\"id\": k, \"prediction_text\": v} \n",
    "        for k, v in final_predictions.items()\n",
    "    ]\n",
    "\n",
    "references = [\n",
    "    {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} \n",
    "    for ex in datasets[\"validation\"]\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=formatted_predictions, references=references)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homeworkï¼šåŠ è½½æœ¬åœ°ä¿å­˜çš„æ¨¡å‹ï¼Œè¿›è¡Œè¯„ä¼°å’Œå†è®­ç»ƒæ›´é«˜çš„ F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = AutoModelForQuestionAnswering.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_trainer = Trainer(\n",
    "    trained_model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53115' max='53115' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53115/53115 1:25:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.532700</td>\n",
       "      <td>1.577670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.393100</td>\n",
       "      <td>2.028740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.368200</td>\n",
       "      <td>2.326906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-1500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-2000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-2500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-3000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-3500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-4000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-4500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-5000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-5500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-6000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-6500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-7000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-7500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-8000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-8500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-9000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-9500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-10000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-10500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-11000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-11500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-12000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-12500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-13000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-13500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-14000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-14500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-15000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-15500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-16000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-16500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-17000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-17500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-18000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-18500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-19000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-19500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-20000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-20500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-21000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-21500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-22000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-22500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-23000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-23500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-24000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-24500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-25000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-25500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-26000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-26500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-27000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-27500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-28000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-28500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-29000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-29500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-30000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-30500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-31000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-31500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-32000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-32500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-33000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-33500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-34000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-34500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-35000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-35500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-36000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-36500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-37000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-37500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-38000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-38500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-39000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-39500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-40000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-40500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-41000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-41500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-42000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-42500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-43000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-43500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-44000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-44500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-45000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-45500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-46000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-46500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-47000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-47500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-48000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-48500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-49000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-49500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-50000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-50500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-51000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-51500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-52000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-52500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert/distilbert-base-uncased-finetuned-squad/checkpoint-53000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=53115, training_loss=0.4066023621172664, metrics={'train_runtime': 5140.1359, 'train_samples_per_second': 51.666, 'train_steps_per_second': 10.333, 'total_flos': 2.602335381127373e+16, 'train_loss': 0.4066023621172664, 'epoch': 3.0})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆåŠŸåŠ è½½æœ¬åœ°æ¨¡å‹: models/distilbert/distilbert-base-uncased-finetuned-squad\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- tokenizer_config.json, vocab.txt ç­‰åˆ†è¯å™¨æ–‡ä»¶\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m     exit(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m datasets \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     28\u001b[0m qa_pipeline \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion-answering\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     30\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     31\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     32\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m     33\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py:2523\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2518\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2519\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2520\u001b[0m )\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2523\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2536\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2537\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2538\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2540\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py:2232\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   2230\u001b[0m builder_cls \u001b[38;5;241m=\u001b[39m get_dataset_builder_class(dataset_module, dataset_name\u001b[38;5;241m=\u001b[39mdataset_name)\n\u001b[1;32m   2231\u001b[0m \u001b[38;5;66;03m# Instantiate the dataset builder\u001b[39;00m\n\u001b[0;32m-> 2232\u001b[0m builder_instance: DatasetBuilder \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mhash\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2239\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2246\u001b[0m builder_instance\u001b[38;5;241m.\u001b[39m_use_legacy_cache_dir_if_possible(dataset_module)\n\u001b[1;32m   2248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "squad_v2 = False  \n",
    "model_save_path = model_dir  \n",
    "dataset_name = \"squad\" \n",
    "validation_split = \"validation\"\n",
    "\n",
    "# 1. åŠ è½½è¯„ä¼°æŒ‡æ ‡\n",
    "metric = evaluate.load(\"squad_v2\" if squad_v2 else \"squad\")\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_save_path)\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_save_path)\n",
    "    print(f\"æˆåŠŸåŠ è½½æœ¬åœ°æ¨¡å‹: {model_save_path}\")\n",
    "except OSError as e:\n",
    "    print(f\"åŠ è½½æ¨¡å‹å¤±è´¥: {e}\")\n",
    "    print(\"è¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œç¡®ä¿åŒ…å«ä»¥ä¸‹æ–‡ä»¶:\")\n",
    "    print(\"- config.json, pytorch_model.bin (æˆ– model.safetensors)\")\n",
    "    print(\"- tokenizer_config.json, vocab.txt ç­‰åˆ†è¯å™¨æ–‡ä»¶\")\n",
    "    exit(1)\n",
    "\n",
    "datasets = load_dataset(dataset_name, split=validation_split)\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "final_predictions = {}\n",
    "print(f\"å¼€å§‹åœ¨éªŒè¯é›†ä¸Šç”Ÿæˆé¢„æµ‹ (å…± {len(datasets)} æ¡æ•°æ®)\")\n",
    "for i, example in enumerate(datasets):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"å·²å¤„ç† {i}/{len(datasets)} æ¡æ•°æ®\")\n",
    "    \n",
    "    result = qa_pipeline(\n",
    "        question=example[\"question\"],\n",
    "        context=example[\"context\"]\n",
    "    )\n",
    "    final_predictions[example[\"id\"]] = result[\"answer\"]\n",
    "\n",
    "if squad_v2:\n",
    "    formatted_predictions = [\n",
    "        {\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0}\n",
    "        for k, v in final_predictions.items()\n",
    "    ]\n",
    "else:\n",
    "    formatted_predictions = [\n",
    "        {\"id\": k, \"prediction_text\": v}\n",
    "        for k, v in final_predictions.items()\n",
    "    ]\n",
    "\n",
    "references = [\n",
    "    {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]}\n",
    "    for ex in datasets\n",
    "]\n",
    "\n",
    "metrics_result = metric.compute(\n",
    "    predictions=formatted_predictions,\n",
    "    references=references\n",
    ")\n",
    "\n",
    "print(\"\\nè¯„ä¼°ç»“æœ:\")\n",
    "print(f\"F1åˆ†æ•°: {metrics_result['f1']:.2f}\")\n",
    "print(f\"ç²¾ç¡®åŒ¹é…ç‡: {metrics_result['exact_match']:.2f}\")\n",
    "\n",
    "if squad_v2:\n",
    "    print(f\"æ— ç­”æ¡ˆç²¾ç¡®åŒ¹é…ç‡: {metrics_result['no_answer_exact_match']:.2f}\")\n",
    "    print(f\"æ— ç­”æ¡ˆF1åˆ†æ•°: {metrics_result['no_answer_f1']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Question Answering on SQUAD",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
